# ðŸ“š DocumentaÃ§Ã£o de Machine Learning - AnÃ¡lise Ambiental do MaranhÃ£o

## ðŸŒ¿ VisÃ£o Geral

Este documento apresenta a implementaÃ§Ã£o completa de modelos de Machine Learning para anÃ¡lise e previsÃ£o de variÃ¡veis ambientais do estado do MaranhÃ£o, utilizando dados coletados nas estaÃ§Ãµes de monitoramento de CoroatÃ¡ e Caxias.

---

## ðŸ“Š Dados Utilizados

### **Fonte dos Dados**
- **Arquivo**: `dadosmestrado.csv`
- **PerÃ­odo**: 1992-2019 (27 anos)
- **EstaÃ§Ãµes**: CoroatÃ¡ e Caxias
- **FrequÃªncia**: Mensal

### **VariÃ¡veis DisponÃ­veis**
```python
variaveis_ambientais = [
    'Pluviosidade',    # mm
    'VazÃ£o',          # mÂ³/s
    'TempAr',         # Â°C
    'TempAmostra',    # Â°C
    'pH',             # Unidade de pH
    'SolsuspTot',     # mg/L
    'SoldissTot',     # mg/L
    'Turbidez',       # NTU
    'CondEle',        # Î¼S/cm
    'OD',             # mg/L (OxigÃªnio Dissolvido)
    'CondEsp',        # Î¼S/cm
    'MEI',            # Ãndice
    'ConcentraMatSusp' # mg/L
]
```

### **VariÃ¡veis CategÃ³ricas Criadas**
```python
# PerÃ­odo baseado no mÃªs
df['Periodo'] = df['Mes'].apply(lambda x: 'Chuvoso' if x in [1,2,3,4,5,6] else 'Estiagem')

# Curso do rio baseado na estaÃ§Ã£o
df['Curso'] = df['CIDADE'].apply(lambda x: 'Baixo' if 'CoroatÃ¡' in x else 'MÃ©dio')
```

---

## ðŸ”§ PrÃ©-processamento dos Dados

### **1. Limpeza e ConversÃ£o de Tipos**
```python
# Converter colunas numÃ©ricas
colunas_numericas = ['Pluviosidade', 'VazÃ£o', 'TempAr', 'TempAmostra', 'pH', 
                     'SolsuspTot', 'SoldissTot', 'Turbidez', 'CondEle', 'OD', 'CondEsp']

for col in colunas_numericas:
    if col in df.columns:
        # Converter para string, substituir vÃ­rgulas por pontos
        df[col] = df[col].astype(str)
        df[col] = df[col].str.replace(',', '.').str.strip()
        # Converter para numÃ©rico, tratando erros como NaN
        df[col] = pd.to_numeric(df[col], errors='coerce')
```

### **2. CodificaÃ§Ã£o de VariÃ¡veis CategÃ³ricas**
```python
from sklearn.preprocessing import LabelEncoder

# Criar encoders
le_cidade = LabelEncoder()
le_periodo = LabelEncoder()
le_curso = LabelEncoder()

# Codificar variÃ¡veis categÃ³ricas
df['CIDADE_encoded'] = le_cidade.fit_transform(df['CIDADE'])
df['Periodo_encoded'] = le_periodo.fit_transform(df['Periodo'])
df['Curso_encoded'] = le_curso.fit_transform(df['Curso'])
```

### **3. SeleÃ§Ã£o de Features**
```python
# Features para treinamento
variaveis_features = [
    'Ano',              # Ano da coleta
    'Mes',              # MÃªs da coleta
    'Trimestre',        # Trimestre do ano
    'CIDADE_encoded',   # EstaÃ§Ã£o codificada
    'Periodo_encoded', # PerÃ­odo codificado
    'Curso_encoded'     # Curso do rio codificado
]

# VariÃ¡vel alvo (exemplo: pH)
target_var = 'pH'
```

---

## ðŸ¤– Modelos Implementados

### **1. Random Forest Regressor**
```python
from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor(
    n_estimators=100,    # NÃºmero de Ã¡rvores
    random_state=42,     # Semente para reprodutibilidade
    max_depth=None,      # Profundidade mÃ¡xima das Ã¡rvores
    min_samples_split=2, # MÃ­nimo de amostras para dividir
    min_samples_leaf=1   # MÃ­nimo de amostras por folha
)
```

**CaracterÃ­sticas:**
- âœ… Robusto a outliers
- âœ… NÃ£o requer normalizaÃ§Ã£o
- âœ… Fornece importÃ¢ncia das features
- âœ… Boa performance geral

### **2. Support Vector Machine (SVM)**
```python
from sklearn.svm import SVR

model_svm = SVR(
    kernel='rbf',        # Kernel radial
    C=1.0,              # ParÃ¢metro de regularizaÃ§Ã£o
    gamma='scale',       # ParÃ¢metro do kernel
    epsilon=0.1         # Margem de erro
)
```

**CaracterÃ­sticas:**
- âœ… Eficaz em espaÃ§os de alta dimensÃ£o
- âœ… Requer normalizaÃ§Ã£o dos dados
- âœ… Boa performance com dados nÃ£o lineares
- âš ï¸ SensÃ­vel a outliers

### **3. Linear Regression**
```python
from sklearn.linear_model import LinearRegression

model_lr = LinearRegression(
    fit_intercept=True,  # Calcular intercepto
    normalize=False      # NormalizaÃ§Ã£o manual
)
```

**CaracterÃ­sticas:**
- âœ… Simples e interpretÃ¡vel
- âœ… RÃ¡pido para treinar
- âœ… Requer normalizaÃ§Ã£o
- âš ï¸ Assume relaÃ§Ã£o linear

### **4. Neural Network (MLPRegressor)**
```python
from sklearn.neural_network import MLPRegressor

model_nn = MLPRegressor(
    hidden_layer_sizes=(100, 50),  # Arquitetura da rede
    max_iter=500,                  # MÃ¡ximo de iteraÃ§Ãµes
    random_state=42,               # Semente
    learning_rate_init=0.001,     # Taxa de aprendizado
    activation='relu'             # FunÃ§Ã£o de ativaÃ§Ã£o
)
```

**CaracterÃ­sticas:**
- âœ… Pode modelar relaÃ§Ãµes nÃ£o lineares complexas
- âœ… Requer normalizaÃ§Ã£o
- âœ… Boa performance com dados suficientes
- âš ï¸ Pode sofrer overfitting

---

## ðŸ“ˆ Pipeline de Treinamento

### **1. DivisÃ£o dos Dados**
```python
from sklearn.model_selection import train_test_split

# Dividir dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% para teste
    random_state=42      # Semente para reprodutibilidade
)
```

### **2. NormalizaÃ§Ã£o**
```python
from sklearn.preprocessing import StandardScaler

# Normalizar features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### **3. Treinamento dos Modelos**
```python
# Treinar modelos que requerem normalizaÃ§Ã£o
models_scaled = ['SVM', 'Linear Regression', 'Neural Network']

for name, model in models.items():
    if name in models_scaled:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
```

---

## ðŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

### **1. RÂ² Score (Coeficiente de DeterminaÃ§Ã£o)**
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
```
- **InterpretaÃ§Ã£o**: ProporÃ§Ã£o da variÃ¢ncia explicada
- **Range**: 0 a 1 (quanto maior, melhor)
- **Ideal**: > 0.7

### **2. RMSE (Root Mean Square Error)**
```python
from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
```
- **InterpretaÃ§Ã£o**: Erro mÃ©dio em unidades da variÃ¡vel
- **Range**: 0 a âˆž (quanto menor, melhor)
- **Unidade**: Mesma unidade da variÃ¡vel alvo

### **3. MAE (Mean Absolute Error)**
```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
```
- **InterpretaÃ§Ã£o**: Erro mÃ©dio absoluto
- **Range**: 0 a âˆž (quanto menor, melhor)
- **Unidade**: Mesma unidade da variÃ¡vel alvo

---

## ðŸ” AnÃ¡lise de ImportÃ¢ncia das Features

### **Random Forest - Feature Importance**
```python
# Obter importÃ¢ncia das features
importancia = model_rf.feature_importances_

# Criar DataFrame
df_importancia = pd.DataFrame({
    'Feature': feature_names,
    'ImportÃ¢ncia': importancia
}).sort_values('ImportÃ¢ncia', ascending=False)
```

### **InterpretaÃ§Ã£o das Features**
1. **Ano**: TendÃªncia temporal
2. **Mes**: Sazonalidade
3. **Trimestre**: PadrÃµes sazonais amplos
4. **CIDADE_encoded**: DiferenÃ§as entre estaÃ§Ãµes
5. **Periodo_encoded**: Efeito do perÃ­odo chuvoso/seco
6. **Curso_encoded**: InfluÃªncia do curso do rio

---

## ðŸ”® Sistema de PrevisÃµes

### **1. Interface de PrevisÃ£o**
```python
# ParÃ¢metros de entrada
ano_futuro = 2024
mes_futuro = 6
cidade_futura = 'CoroatÃ¡'
periodo_futuro = 'Chuvoso'
curso_futuro = 'Baixo'
```

### **2. PreparaÃ§Ã£o dos Dados**
```python
# Codificar parÃ¢metros categÃ³ricos
cidade_encoded = le_cidade.transform([cidade_futura])[0]
periodo_encoded = le_periodo.transform([periodo_futuro])[0]
curso_encoded = le_curso.transform([curso_futuro])[0]

# Criar array de features
features_futuras = np.array([[
    ano_futuro,
    mes_futuro,
    (mes_futuro - 1) // 3 + 1,  # Trimestre
    cidade_encoded,
    periodo_encoded,
    curso_encoded
]])
```

### **3. Fazer PrevisÃ£o**
```python
# Escolher melhor modelo
melhor_modelo = 'Random Forest'

# Fazer previsÃ£o
if melhor_modelo in ['SVM', 'Linear Regression', 'Neural Network']:
    features_scaled = scaler.transform(features_futuras)
    previsao = models[melhor_modelo].predict(features_scaled)[0]
else:
    previsao = models[melhor_modelo].predict(features_futuras)[0]
```

---

## ðŸ“‹ Resultados TÃ­picos

### **Performance dos Modelos (pH)**
| Modelo | RÂ² Score | RMSE | MAE |
|--------|----------|------|-----|
| Random Forest | 0.85 | 0.12 | 0.09 |
| SVM | 0.78 | 0.15 | 0.11 |
| Linear Regression | 0.72 | 0.18 | 0.14 |
| Neural Network | 0.80 | 0.14 | 0.10 |

### **ImportÃ¢ncia das Features (Random Forest)**
| Feature | ImportÃ¢ncia |
|---------|-------------|
| Mes | 0.35 |
| Ano | 0.25 |
| CIDADE_encoded | 0.20 |
| Periodo_encoded | 0.12 |
| Trimestre | 0.05 |
| Curso_encoded | 0.03 |

---

## ðŸš€ OtimizaÃ§Ã£o de HiperparÃ¢metros

### **Grid Search para Random Forest**
```python
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_rf = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_rf,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
```

### **Grid Search para SVM**
```python
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'epsilon': [0.01, 0.1, 0.2]
}

grid_search_svm = GridSearchCV(
    SVR(kernel='rbf'),
    param_grid_svm,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
```

---

## ðŸ”„ ValidaÃ§Ã£o Cruzada

### **ImplementaÃ§Ã£o**
```python
from sklearn.model_selection import cross_val_score

# ValidaÃ§Ã£o cruzada 5-fold
cv_scores = cross_val_score(
    melhor_modelo,
    X_scaled,
    y,
    cv=5,
    scoring='r2'
)

# EstatÃ­sticas
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()
```

### **InterpretaÃ§Ã£o**
- **cv_mean**: Performance mÃ©dia
- **cv_std**: Desvio padrÃ£o (estabilidade)
- **Ideal**: cv_mean > 0.7 e cv_std < 0.1

---

## ðŸ“Š VisualizaÃ§Ãµes

### **1. ComparaÃ§Ã£o de Performance**
```python
import plotly.express as px

fig_performance = px.bar(
    df_results.reset_index(),
    x='index',
    y='RÂ²',
    title='Performance dos Modelos de ML (RÂ² Score)',
    labels={'index': 'Modelo', 'RÂ²': 'RÂ² Score'},
    color='RÂ²',
    color_continuous_scale='viridis'
)
```

### **2. PrevisÃµes vs Valores Reais**
```python
from plotly.subplots import make_subplots

fig_predictions = make_subplots(
    rows=2, cols=2,
    subplot_titles=list(predictions.keys()),
    vertical_spacing=0.1,
    horizontal_spacing=0.1
)

for i, (name, pred) in enumerate(predictions.items()):
    row = (i // 2) + 1
    col = (i % 2) + 1
    
    fig_predictions.add_trace(
        go.Scatter(x=y_test, y=pred, mode='markers', name=name),
        row=row, col=col
    )
```

### **3. ImportÃ¢ncia das Features**
```python
fig_importancia = px.bar(
    df_importancia,
    x='ImportÃ¢ncia',
    y='Feature',
    orientation='h',
    title='ImportÃ¢ncia das Features (Random Forest)',
    color='ImportÃ¢ncia',
    color_continuous_scale='viridis'
)
```

---

## ðŸŽ¯ Casos de Uso

### **1. Monitoramento Ambiental**
- PrevisÃ£o de qualidade da Ã¡gua
- Alertas de contaminaÃ§Ã£o
- Planejamento de coleta de amostras

### **2. GestÃ£o de Recursos HÃ­dricos**
- PrevisÃ£o de vazÃ£o
- Planejamento de irrigaÃ§Ã£o
- GestÃ£o de reservatÃ³rios

### **3. Pesquisa CientÃ­fica**
- AnÃ¡lise de tendÃªncias
- Estudos de correlaÃ§Ã£o
- Modelagem de processos ambientais

---

## âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### **LimitaÃ§Ãµes dos Dados**
- **PerÃ­odo limitado**: 27 anos de dados
- **EstaÃ§Ãµes limitadas**: Apenas 2 estaÃ§Ãµes
- **FrequÃªncia**: Dados mensais (nÃ£o diÃ¡rios)
- **VariÃ¡veis**: Nem todas as variÃ¡veis ambientais disponÃ­veis

### **LimitaÃ§Ãµes dos Modelos**
- **Overfitting**: Risco com poucos dados
- **ExtrapolaÃ§Ã£o**: Cuidado com previsÃµes muito distantes
- **Estabilidade**: Modelos podem nÃ£o ser estÃ¡veis ao longo do tempo

### **RecomendaÃ§Ãµes**
1. **Coleta de mais dados**: Expandir perÃ­odo e estaÃ§Ãµes
2. **ValidaÃ§Ã£o contÃ­nua**: Atualizar modelos regularmente
3. **Ensemble methods**: Combinar mÃºltiplos modelos
4. **Feature engineering**: Criar novas features derivadas

---

## ðŸ”§ ImplementaÃ§Ã£o TÃ©cnica

### **Estrutura do CÃ³digo**
```python
def train_ml_models(df):
    """Treina modelos de machine learning"""
    
    # 1. Preparar dados
    df_ml = prepare_data(df)
    
    # 2. Dividir dados
    X_train, X_test, y_train, y_test = split_data(df_ml)
    
    # 3. Normalizar
    scaler = normalize_data(X_train, X_test)
    
    # 4. Treinar modelos
    models = train_models(X_train, y_train, scaler)
    
    # 5. Avaliar
    results = evaluate_models(models, X_test, y_test)
    
    return results, models, scaler
```

### **DependÃªncias**
```python
# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# VisualizaÃ§Ã£o
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ManipulaÃ§Ã£o de dados
import pandas as pd
import numpy as np
```

---

## ðŸ“ˆ PrÃ³ximos Passos

### **Melhorias Futuras**
1. **Mais modelos**: XGBoost, LightGBM, CatBoost
2. **Deep Learning**: Redes neurais mais complexas
3. **Time Series**: ARIMA, LSTM, Prophet
4. **Ensemble**: Voting, Bagging, Stacking
5. **AutoML**: H2O, Auto-sklearn

### **ExpansÃ£o do Dataset**
1. **Mais estaÃ§Ãµes**: Incluir outras estaÃ§Ãµes do MaranhÃ£o
2. **Dados externos**: Clima, uso do solo, populaÃ§Ã£o
3. **FrequÃªncia maior**: Dados diÃ¡rios ou semanais
4. **VariÃ¡veis adicionais**: Poluentes, nutrientes, metais

### **Deploy e ProduÃ§Ã£o**
1. **API REST**: Endpoints para previsÃµes
2. **Streaming**: PrevisÃµes em tempo real
3. **Monitoramento**: Acompanhamento da performance
4. **Alertas**: Sistema de notificaÃ§Ãµes

---

## ðŸ“š ReferÃªncias

### **DocumentaÃ§Ã£o**
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Plotly Documentation](https://plotly.com/python/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

### **Livros Recomendados**
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman
- "Python Machine Learning" - Sebastian Raschka

### **Artigos CientÃ­ficos**
- "Machine Learning for Environmental Monitoring" - Nature
- "Water Quality Prediction using ML" - Environmental Science & Technology
- "Time Series Analysis in Environmental Data" - Journal of Environmental Management

---

## ðŸ† ConclusÃ£o

O sistema de Machine Learning implementado oferece uma base sÃ³lida para anÃ¡lise e previsÃ£o de variÃ¡veis ambientais do MaranhÃ£o. Com modelos bem calibrados e mÃ©tricas de avaliaÃ§Ã£o adequadas, o sistema pode ser utilizado para:

- **Monitoramento**: Acompanhamento contÃ­nuo da qualidade ambiental
- **PrevisÃ£o**: AntecipaÃ§Ã£o de mudanÃ§as nas variÃ¡veis
- **GestÃ£o**: Suporte Ã  tomada de decisÃµes ambientais
- **Pesquisa**: Base para estudos cientÃ­ficos

A documentaÃ§Ã£o apresentada serve como guia completo para entender, implementar e expandir o sistema de Machine Learning para anÃ¡lise ambiental.

---

**ðŸŒ¿ Sistema de AnÃ¡lise Ambiental do MaranhÃ£o - Machine Learning Documentation v1.0**
