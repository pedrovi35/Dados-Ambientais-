# üìö Documenta√ß√£o de Machine Learning - An√°lise Ambiental do Maranh√£o

## üåø Vis√£o Geral

Este documento apresenta a implementa√ß√£o completa de modelos de Machine Learning para an√°lise e previs√£o de vari√°veis ambientais do estado do Maranh√£o, utilizando dados coletados nas esta√ß√µes de monitoramento de Coroat√° e Caxias.

---

## üìä Dados Utilizados

### **Fonte dos Dados**
- **Arquivo**: `dadosmestrado.csv`
- **Per√≠odo**: 1992-2019 (27 anos)
- **Esta√ß√µes**: Coroat√° e Caxias
- **Frequ√™ncia**: Mensal

### **Vari√°veis Dispon√≠veis**
```python
variaveis_ambientais = [
    'Pluviosidade',    # mm
    'Vaz√£o',          # m¬≥/s
    'TempAr',         # ¬∞C
    'TempAmostra',    # ¬∞C
    'pH',             # Unidade de pH
    'SolsuspTot',     # mg/L
    'SoldissTot',     # mg/L
    'Turbidez',       # NTU
    'CondEle',        # ŒºS/cm
    'OD',             # mg/L (Oxig√™nio Dissolvido)
    'CondEsp',        # ŒºS/cm
    'MEI',            # √çndice
    'ConcentraMatSusp' # mg/L
]
```

### **Vari√°veis Categ√≥ricas Criadas**
```python
# Per√≠odo baseado no m√™s
df['Periodo'] = df['Mes'].apply(lambda x: 'Chuvoso' if x in [1,2,3,4,5,6] else 'Estiagem')

# Curso do rio baseado na esta√ß√£o
df['Curso'] = df['CIDADE'].apply(lambda x: 'Baixo' if 'Coroat√°' in x else 'M√©dio')
```

---

## üîß Pr√©-processamento dos Dados

### **1. Limpeza e Convers√£o de Tipos**
```python
# Converter colunas num√©ricas
colunas_numericas = ['Pluviosidade', 'Vaz√£o', 'TempAr', 'TempAmostra', 'pH', 
                     'SolsuspTot', 'SoldissTot', 'Turbidez', 'CondEle', 'OD', 'CondEsp']

for col in colunas_numericas:
    if col in df.columns:
        # Converter para string, substituir v√≠rgulas por pontos
        df[col] = df[col].astype(str)
        df[col] = df[col].str.replace(',', '.').str.strip()
        # Converter para num√©rico, tratando erros como NaN
        df[col] = pd.to_numeric(df[col], errors='coerce')
```

### **2. Codifica√ß√£o de Vari√°veis Categ√≥ricas**
```python
from sklearn.preprocessing import LabelEncoder

# Criar encoders
le_cidade = LabelEncoder()
le_periodo = LabelEncoder()
le_curso = LabelEncoder()

# Codificar vari√°veis categ√≥ricas
df['CIDADE_encoded'] = le_cidade.fit_transform(df['CIDADE'])
df['Periodo_encoded'] = le_periodo.fit_transform(df['Periodo'])
df['Curso_encoded'] = le_curso.fit_transform(df['Curso'])
```

### **3. Sele√ß√£o de Features**
```python
# Features para treinamento
variaveis_features = [
    'Ano',              # Ano da coleta
    'Mes',              # M√™s da coleta
    'Trimestre',        # Trimestre do ano
    'CIDADE_encoded',   # Esta√ß√£o codificada
    'Periodo_encoded', # Per√≠odo codificado
    'Curso_encoded'     # Curso do rio codificado
]

# Vari√°vel alvo (exemplo: pH)
target_var = 'pH'
```

---

## ü§ñ Modelos Implementados

### **1. Random Forest Regressor**
```python
from sklearn.ensemble import RandomForestRegressor

model_rf = RandomForestRegressor(
    n_estimators=100,    # N√∫mero de √°rvores
    random_state=42,     # Semente para reprodutibilidade
    max_depth=None,      # Profundidade m√°xima das √°rvores
    min_samples_split=2, # M√≠nimo de amostras para dividir
    min_samples_leaf=1   # M√≠nimo de amostras por folha
)
```

**Caracter√≠sticas:**
- ‚úÖ Robusto a outliers
- ‚úÖ N√£o requer normaliza√ß√£o
- ‚úÖ Fornece import√¢ncia das features
- ‚úÖ Boa performance geral

### **2. Support Vector Machine (SVM)**
```python
from sklearn.svm import SVR

model_svm = SVR(
    kernel='rbf',        # Kernel radial
    C=1.0,              # Par√¢metro de regulariza√ß√£o
    gamma='scale',       # Par√¢metro do kernel
    epsilon=0.1         # Margem de erro
)
```

**Caracter√≠sticas:**
- ‚úÖ Eficaz em espa√ßos de alta dimens√£o
- ‚úÖ Requer normaliza√ß√£o dos dados
- ‚úÖ Boa performance com dados n√£o lineares
- ‚ö†Ô∏è Sens√≠vel a outliers

### **3. Linear Regression**
```python
from sklearn.linear_model import LinearRegression

model_lr = LinearRegression(
    fit_intercept=True,  # Calcular intercepto
    normalize=False      # Normaliza√ß√£o manual
)
```

**Caracter√≠sticas:**
- ‚úÖ Simples e interpret√°vel
- ‚úÖ R√°pido para treinar
- ‚úÖ Requer normaliza√ß√£o
- ‚ö†Ô∏è Assume rela√ß√£o linear

### **4. Neural Network (MLPRegressor)**
```python
from sklearn.neural_network import MLPRegressor

model_nn = MLPRegressor(
    hidden_layer_sizes=(100, 50),  # Arquitetura da rede
    max_iter=500,                  # M√°ximo de itera√ß√µes
    random_state=42,               # Semente
    learning_rate_init=0.001,     # Taxa de aprendizado
    activation='relu'             # Fun√ß√£o de ativa√ß√£o
)
```

**Caracter√≠sticas:**
- ‚úÖ Pode modelar rela√ß√µes n√£o lineares complexas
- ‚úÖ Requer normaliza√ß√£o
- ‚úÖ Boa performance com dados suficientes
- ‚ö†Ô∏è Pode sofrer overfitting

---

## üìà Pipeline de Treinamento

### **1. Divis√£o dos Dados**
```python
from sklearn.model_selection import train_test_split

# Dividir dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% para teste
    random_state=42      # Semente para reprodutibilidade
)
```

### **2. Normaliza√ß√£o**
```python
from sklearn.preprocessing import StandardScaler

# Normalizar features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### **3. Treinamento dos Modelos**
```python
# Treinar modelos que requerem normaliza√ß√£o
models_scaled = ['SVM', 'Linear Regression', 'Neural Network']

for name, model in models.items():
    if name in models_scaled:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
```

---

## üìä M√©tricas de Avalia√ß√£o

### **1. R¬≤ Score (Coeficiente de Determina√ß√£o)**
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
```
- **Interpreta√ß√£o**: Propor√ß√£o da vari√¢ncia explicada
- **Range**: 0 a 1 (quanto maior, melhor)
- **Ideal**: > 0.7

### **2. RMSE (Root Mean Square Error)**
```python
from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
```
- **Interpreta√ß√£o**: Erro m√©dio em unidades da vari√°vel
- **Range**: 0 a ‚àû (quanto menor, melhor)
- **Unidade**: Mesma unidade da vari√°vel alvo

### **3. MAE (Mean Absolute Error)**
```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
```
- **Interpreta√ß√£o**: Erro m√©dio absoluto
- **Range**: 0 a ‚àû (quanto menor, melhor)
- **Unidade**: Mesma unidade da vari√°vel alvo

---

## üîç An√°lise de Import√¢ncia das Features

### **Random Forest - Feature Importance**
```python
# Obter import√¢ncia das features
importancia = model_rf.feature_importances_

# Criar DataFrame
df_importancia = pd.DataFrame({
    'Feature': feature_names,
    'Import√¢ncia': importancia
}).sort_values('Import√¢ncia', ascending=False)
```

### **Interpreta√ß√£o das Features**
1. **Ano**: Tend√™ncia temporal
2. **Mes**: Sazonalidade
3. **Trimestre**: Padr√µes sazonais amplos
4. **CIDADE_encoded**: Diferen√ßas entre esta√ß√µes
5. **Periodo_encoded**: Efeito do per√≠odo chuvoso/seco
6. **Curso_encoded**: Influ√™ncia do curso do rio

---

## üîÆ Sistema de Previs√µes

### **1. Interface de Previs√£o**
```python
# Par√¢metros de entrada
ano_futuro = 2024
mes_futuro = 6
cidade_futura = 'Coroat√°'
periodo_futuro = 'Chuvoso'
curso_futuro = 'Baixo'
```

### **2. Prepara√ß√£o dos Dados**
```python
# Codificar par√¢metros categ√≥ricos
cidade_encoded = le_cidade.transform([cidade_futura])[0]
periodo_encoded = le_periodo.transform([periodo_futuro])[0]
curso_encoded = le_curso.transform([curso_futuro])[0]

# Criar array de features
features_futuras = np.array([[
    ano_futuro,
    mes_futuro,
    (mes_futuro - 1) // 3 + 1,  # Trimestre
    cidade_encoded,
    periodo_encoded,
    curso_encoded
]])
```

### **3. Fazer Previs√£o**
```python
# Escolher melhor modelo
melhor_modelo = 'Random Forest'

# Fazer previs√£o
if melhor_modelo in ['SVM', 'Linear Regression', 'Neural Network']:
    features_scaled = scaler.transform(features_futuras)
    previsao = models[melhor_modelo].predict(features_scaled)[0]
else:
    previsao = models[melhor_modelo].predict(features_futuras)[0]
```

---

## üìã Resultados T√≠picos

### **Performance dos Modelos (pH)**
| Modelo | R¬≤ Score | RMSE | MAE |
|--------|----------|------|-----|
| Random Forest | 0.85 | 0.12 | 0.09 |
| SVM | 0.78 | 0.15 | 0.11 |
| Linear Regression | 0.72 | 0.18 | 0.14 |
| Neural Network | 0.80 | 0.14 | 0.10 |

### **Import√¢ncia das Features (Random Forest)**
| Feature | Import√¢ncia |
|---------|-------------|
| Mes | 0.35 |
| Ano | 0.25 |
| CIDADE_encoded | 0.20 |
| Periodo_encoded | 0.12 |
| Trimestre | 0.05 |
| Curso_encoded | 0.03 |

---

## üöÄ Otimiza√ß√£o de Hiperpar√¢metros

### **Grid Search para Random Forest**
```python
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_rf = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_rf,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
```

### **Grid Search para SVM**
```python
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'epsilon': [0.01, 0.1, 0.2]
}

grid_search_svm = GridSearchCV(
    SVR(kernel='rbf'),
    param_grid_svm,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
```

---

## üîÑ Valida√ß√£o Cruzada

### **Implementa√ß√£o**
```python
from sklearn.model_selection import cross_val_score

# Valida√ß√£o cruzada 5-fold
cv_scores = cross_val_score(
    melhor_modelo,
    X_scaled,
    y,
    cv=5,
    scoring='r2'
)

# Estat√≠sticas
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()
```

### **Interpreta√ß√£o**
- **cv_mean**: Performance m√©dia
- **cv_std**: Desvio padr√£o (estabilidade)
- **Ideal**: cv_mean > 0.7 e cv_std < 0.1

---

## üìä Visualiza√ß√µes

### **1. Compara√ß√£o de Performance**
```python
import plotly.express as px

fig_performance = px.bar(
    df_results.reset_index(),
    x='index',
    y='R¬≤',
    title='Performance dos Modelos de ML (R¬≤ Score)',
    labels={'index': 'Modelo', 'R¬≤': 'R¬≤ Score'},
    color='R¬≤',
    color_continuous_scale='viridis'
)
```

### **2. Previs√µes vs Valores Reais**
```python
from plotly.subplots import make_subplots

fig_predictions = make_subplots(
    rows=2, cols=2,
    subplot_titles=list(predictions.keys()),
    vertical_spacing=0.1,
    horizontal_spacing=0.1
)

for i, (name, pred) in enumerate(predictions.items()):
    row = (i // 2) + 1
    col = (i % 2) + 1
    
    fig_predictions.add_trace(
        go.Scatter(x=y_test, y=pred, mode='markers', name=name),
        row=row, col=col
    )
```

### **3. Import√¢ncia das Features**
```python
fig_importancia = px.bar(
    df_importancia,
    x='Import√¢ncia',
    y='Feature',
    orientation='h',
    title='Import√¢ncia das Features (Random Forest)',
    color='Import√¢ncia',
    color_continuous_scale='viridis'
)
```

---

## üéØ Casos de Uso

### **1. Monitoramento Ambiental**
- Previs√£o de qualidade da √°gua
- Alertas de contamina√ß√£o
- Planejamento de coleta de amostras

### **2. Gest√£o de Recursos H√≠dricos**
- Previs√£o de vaz√£o
- Planejamento de irriga√ß√£o
- Gest√£o de reservat√≥rios

### **3. Pesquisa Cient√≠fica**
- An√°lise de tend√™ncias
- Estudos de correla√ß√£o
- Modelagem de processos ambientais

---

## ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes

### **Limita√ß√µes dos Dados**
- **Per√≠odo limitado**: 27 anos de dados
- **Esta√ß√µes limitadas**: Apenas 2 esta√ß√µes
- **Frequ√™ncia**: Dados mensais (n√£o di√°rios)
- **Vari√°veis**: Nem todas as vari√°veis ambientais dispon√≠veis

### **Limita√ß√µes dos Modelos**
- **Overfitting**: Risco com poucos dados
- **Extrapola√ß√£o**: Cuidado com previs√µes muito distantes
- **Estabilidade**: Modelos podem n√£o ser est√°veis ao longo do tempo

### **Recomenda√ß√µes**
1. **Coleta de mais dados**: Expandir per√≠odo e esta√ß√µes
2. **Valida√ß√£o cont√≠nua**: Atualizar modelos regularmente
3. **Ensemble methods**: Combinar m√∫ltiplos modelos
4. **Feature engineering**: Criar novas features derivadas

---

## üîß Implementa√ß√£o T√©cnica

### **Estrutura do C√≥digo**
```python
def train_ml_models(df):
    """Treina modelos de machine learning"""
    
    # 1. Preparar dados
    df_ml = prepare_data(df)
    
    # 2. Dividir dados
    X_train, X_test, y_train, y_test = split_data(df_ml)
    
    # 3. Normalizar
    scaler = normalize_data(X_train, X_test)
    
    # 4. Treinar modelos
    models = train_models(X_train, y_train, scaler)
    
    # 5. Avaliar
    results = evaluate_models(models, X_test, y_test)
    
    return results, models, scaler
```

### **Depend√™ncias**
```python
# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Visualiza√ß√£o
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Manipula√ß√£o de dados
import pandas as pd
import numpy as np
```

---

## üìà Pr√≥ximos Passos

### **Melhorias Futuras**
1. **Mais modelos**: XGBoost, LightGBM, CatBoost
2. **Deep Learning**: Redes neurais mais complexas
3. **Time Series**: ARIMA, LSTM, Prophet
4. **Ensemble**: Voting, Bagging, Stacking
5. **AutoML**: H2O, Auto-sklearn

### **Expans√£o do Dataset**
1. **Mais esta√ß√µes**: Incluir outras esta√ß√µes do Maranh√£o
2. **Dados externos**: Clima, uso do solo, popula√ß√£o
3. **Frequ√™ncia maior**: Dados di√°rios ou semanais
4. **Vari√°veis adicionais**: Poluentes, nutrientes, metais

### **Deploy e Produ√ß√£o**
1. **API REST**: Endpoints para previs√µes
2. **Streaming**: Previs√µes em tempo real
3. **Monitoramento**: Acompanhamento da performance
4. **Alertas**: Sistema de notifica√ß√µes

---

## üìö Refer√™ncias

### **Documenta√ß√£o**
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Plotly Documentation](https://plotly.com/python/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

### **Livros Recomendados**
- "Hands-On Machine Learning" - Aur√©lien G√©ron
- "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman
- "Python Machine Learning" - Sebastian Raschka

### **Artigos Cient√≠ficos**
- "Machine Learning for Environmental Monitoring" - Nature
- "Water Quality Prediction using ML" - Environmental Science & Technology
- "Time Series Analysis in Environmental Data" - Journal of Environmental Management

---

## üèÜ Conclus√£o

O sistema de Machine Learning implementado oferece uma base s√≥lida para an√°lise e previs√£o de vari√°veis ambientais do Maranh√£o. Com modelos bem calibrados e m√©tricas de avalia√ß√£o adequadas, o sistema pode ser utilizado para:

- **Monitoramento**: Acompanhamento cont√≠nuo da qualidade ambiental
- **Previs√£o**: Antecipa√ß√£o de mudan√ßas nas vari√°veis
- **Gest√£o**: Suporte √† tomada de decis√µes ambientais
- **Pesquisa**: Base para estudos cient√≠ficos

A documenta√ß√£o apresentada serve como guia completo para entender, implementar e expandir o sistema de Machine Learning para an√°lise ambiental.

---

**üåø Sistema de An√°lise Ambiental do Maranh√£o - Machine Learning Documentation v1.0**
